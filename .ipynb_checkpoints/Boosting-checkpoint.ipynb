{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelisation d'un algorithme de boosting\n",
    "\n",
    "Jacques Song et Tristan Loisel, rendu le 03/02/2020 \n",
    "\n",
    "Eléments logiciels pour traitement de données massives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, nous présentons un algorithme de boosting, codé en `python`, que nous parallelisons à l'aide du package `ray`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des packages nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction des sets d'entraînement et de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons choisi ici de travailler avec les données \"mnist_784\", qui contiennent des chiffres manuscrits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml(name=\"mnist_784\", return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAANN0lEQVR4nO3df4wc9XnH8c/H5nwh5kdswPRkLJsSI5W0jdOeDC0opUGJiKPKEClt3Iq4EsJUwihRSBVEqob+U1lVyY+WlPYCVtyKkuYXwRFWG8tKZUUFh4Ma/6gpUOpgxyc7yCg2IOyz/fSPG6qLuZ2925nd2dzzfkmr3Z1ndufR2p+b3fnO7tcRIQCz35ymGwDQG4QdSIKwA0kQdiAJwg4kcU4vNzbPg/EOze/lJoFU3tTrOhknPFWtUtht3yjpy5LmSnowIjaUrf8OzdfVvqHKJgGU2BHbWtY6fhtve66kr0j6sKSrJK2xfVWnzwegu6p8Zl8p6cWIeCkiTkr6uqTV9bQFoG5Vwr5Y0oFJ9w8Wy36O7XW2R22PjutEhc0BqKJK2Kc6CPC2c28jYiQihiNieECDFTYHoIoqYT8oacmk+5dJOlStHQDdUiXsT0labvty2/MkfVzS5nraAlC3jofeIuKU7fWS/k0TQ28bI2JvbZ0BqFWlcfaI2CJpS029AOgiTpcFkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IImeTtmMXzz/u+G3SuvPf+KB0vqR06+3rN386U+XPva8b+4orWNm2LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKMsyf3xkevLq3vveX+0vrpcGn9ojnntqyNfeRU6WOXf7O0jBmqFHbb+yUdl3Ra0qmIGK6jKQD1q2PP/rsR8UoNzwOgi/jMDiRRNewh6fu2n7a9bqoVbK+zPWp7dFwnKm4OQKeqvo2/NiIO2V4kaavt5yJi++QVImJE0ogkXeCFUXF7ADpUac8eEYeK6yOSHpW0so6mANSv47Dbnm/7/LduS/qQpD11NQagXlXexl8q6VHbbz3PP0fEv9bSFWoz98orSutfuu9vS+tzOBVj1uj4XzIiXpL03hp7AdBFDL0BSRB2IAnCDiRB2IEkCDuQBOMqs9zzt19SWl8xr7n/Auf/52Bj286IPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4+yw379Vqf8+fbPNLYte0GSr/9zcHWtaG/m609LH8rFG92LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKMs89yS797tLT+kQ/8Xmn9x9uXltb33FY+pfPygZ+1rB340/JJfy/7y/8orWNm2LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKMs89yZ/Y8V77CB8rLg3eWj7O3s3juO1vWnrnjy6WPfc9Fd5bWr7jryY56yqrtnt32RttHbO+ZtGyh7a22XyiuF3S3TQBVTedt/Nck3XjWsrslbYuI5ZK2FfcB9LG2YY+I7ZLOPudytaRNxe1Nkm6quS8ANev0AN2lETEmScX1olYr2l5ne9T26Lja/KAZgK7p+tH4iBiJiOGIGB4QE/kBTek07IdtD0lScX2kvpYAdEOnYd8saW1xe62kx+ppB0C3tB1nt/2IpOslXWz7oKTPS9og6Ru2b5X0sqSPdbNJdM85S5eU1n/zj3Z1b9uaW1r/1kfLx+E/e9fVdbYz67UNe0SsaVG6oeZeAHQRp8sCSRB2IAnCDiRB2IEkCDuQBF9xTe70JReW1keWcArFbMGeHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJx9lpszf35p/eUPXlDp+Y+debO0/i/Hl7es3XbhgUrbxsywZweSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBhnn+VOv/fdpfVd6++v9PwP/uzXSutbPtN6TujbHvqHStvGzLBnB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkGGef5Y4vO7fS4/eOnyytb73tutL64Mk3Km0f9Wm7Z7e90fYR23smLbvX9k9s7ywuq7rbJoCqpvM2/muSbpxi+RcjYkVx2VJvWwDq1jbsEbFd0tEe9AKgi6ocoFtve1fxNn9Bq5Vsr7M9ant0XCcqbA5AFZ2G/QFJV0haIWlM0n2tVoyIkYgYjojhAQ12uDkAVXUU9og4HBGnI+KMpK9KWllvWwDq1lHYbQ9NunuzpD2t1gXQH9qOs9t+RNL1ki62fVDS5yVdb3uFpJC0X9LtXewRFbzre3tL68N/8Iel9TPh0vovPfFsaX3/X/x2ab3Mg6+8v80aHAOaibZhj4g1Uyx+qAu9AOgiTpcFkiDsQBKEHUiCsANJEHYgCb7iOsudOX68tL5o9XNd3f7S617u+LGP7/z10vqVeqrj586IPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4Oyo55/KlpfW/effDLWun2vxy0dLvdtQSWmDPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM6OSo5eM1RaX3bOO1vWnmzzS9CDj/N99TqxZweSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBhnr8Gc+fNL6547t7R++tixOtuZmTnlvb1+83BpfdOG+0rrpzSvZe2Wx9eXPna5dpTWMTNt9+y2l9j+ge19tvfa/mSxfKHtrbZfKK4XdL9dAJ2aztv4U5LuiohfkXSNpDtsXyXpbknbImK5pG3FfQB9qm3YI2IsIp4pbh+XtE/SYkmrJW0qVtsk6aZuNQmguhkdoLO9TNL7JO2QdGlEjEkTfxAkLWrxmHW2R22PjqvNydAAumbaYbd9nqRvS/pUREz7iFJEjETEcEQMD7T5gUEA3TOtsNse0ETQH46I7xSLD9seKupDko50p0UAdWg79Gbbkh6StC8ivjCptFnSWkkbiuvHutLhL4BVPzpYWh+cM15a/8pI9w53nDy/vH7Nqt2l9QeXPNBmC+eWVh9/47yWteV3MrTWS9MZZ79W0i2SdtveWSy7RxMh/4btWyW9LOlj3WkRQB3ahj0ifijJLco31NsOgG7hdFkgCcIOJEHYgSQIO5AEYQeS4CuuNXjlVOuxZEn684vLx7Jv/cz9dbZTqzOK0vqXXr2ytP69z7UesDlXP+qoJ3SGPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4ew2e+JPyn1teec97Kj3/0gtfLa1/YuiJlrW/P/A7pY/96evlP4P92rMXldaX/VnrbUuMpfcT9uxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kIQjyr+vXKcLvDCuNj9IC3TLjtimY3F0yl+DZs8OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0m0DbvtJbZ/YHuf7b22P1ksv9f2T2zvLC6rut8ugE5N58crTkm6KyKesX2+pKdtby1qX4yIv+5eewDqMp352cckjRW3j9veJ2lxtxsDUK8ZfWa3vUzS+yTtKBatt73L9kbbC1o8Zp3tUduj4zpRqVkAnZt22G2fJ+nbkj4VEcckPSDpCkkrNLHnv2+qx0XESEQMR8TwgAZraBlAJ6YVdtsDmgj6wxHxHUmKiMMRcToizkj6qqSV3WsTQFXTORpvSQ9J2hcRX5i0fGjSajdL2lN/ewDqMp2j8ddKukXSbts7i2X3SFpje4WkkLRf0u1d6RBALaZzNP6Hkqb6fuyW+tsB0C2cQQckQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiip1M22/6ppB9PWnSxpFd61sDM9Gtv/dqXRG+dqrO3pRFxyVSFnob9bRu3RyNiuLEGSvRrb/3al0RvnepVb7yNB5Ig7EASTYd9pOHtl+nX3vq1L4neOtWT3hr9zA6gd5reswPoEcIOJNFI2G3faPu/bb9o++4memjF9n7bu4tpqEcb7mWj7SO290xattD2VtsvFNdTzrHXUG99MY13yTTjjb52TU9/3vPP7LbnSnpe0gclHZT0lKQ1EfFfPW2kBdv7JQ1HROMnYNh+v6TXJP1jRPxqseyvJB2NiA3FH8oFEfHZPuntXkmvNT2NdzFb0dDkacYl3STpj9Xga1fS1++rB69bE3v2lZJejIiXIuKkpK9LWt1AH30vIrZLOnrW4tWSNhW3N2niP0vPteitL0TEWEQ8U9w+LumtacYbfe1K+uqJJsK+WNKBSfcPqr/mew9J37f9tO11TTczhUsjYkya+M8jaVHD/Zyt7TTevXTWNON989p1Mv15VU2EfaqppPpp/O/aiPgNSR+WdEfxdhXTM61pvHtlimnG+0Kn059X1UTYD0paMun+ZZIONdDHlCLiUHF9RNKj6r+pqA+/NYNucX2k4X7+Xz9N4z3VNOPqg9euyenPmwj7U5KW277c9jxJH5e0uYE+3sb2/OLAiWzPl/Qh9d9U1JslrS1ur5X0WIO9/Jx+mca71TTjavi1a3z684jo+UXSKk0ckf8fSZ9roocWff2ypGeLy96me5P0iCbe1o1r4h3RrZIukrRN0gvF9cI+6u2fJO2WtEsTwRpqqLfrNPHRcJekncVlVdOvXUlfPXndOF0WSIIz6IAkCDuQBGEHkiDsQBKEHUiCsANJEHYgif8DcXbZY7NMEskAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.reshape(X_train[0], (28, 28)))\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'algorithme:\n",
    "\n",
    "L'algorithme que nous implémentons est le suivant, il a été présenté par Aleksandar LAZAREVIC et Zoran OBRADOVIC dans *Boosting Algorithms for Parallel and Distributed Learning, Distributed and Parallel Databases, 11, 203–229, 2002*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Entrée: un set de données labellisées $S = \\{(X_i,Y_i)\\}_{i \\in \\{1,...,n\\}}$, avec pour chaque $i$, $y_i \\in \\{0,...,9\\}$\n",
    "\n",
    "Initialisation: création \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;-d'un ensemble $B=\\{(i,y), i \\in \\{1,...,n\\}, y_i \\neq y\\}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;-d'une densité de probabilité sur S, $\\mu_1$, telle que $\\mu_1(i)=\\frac{1}{n}$ pour chaque $i \\in \\{1,...,n\\}$.\n",
    "\n",
    "Pour chaque $t \\in \\{1,...,T\\}$:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;Pour chaque processeur $j \\{1,...,J\\}$:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-Tirer un échantillon à partir de la densité $\\mu_t$\n",
    "        \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-Entraîner un prédicteur $P_{t,j}$ en utilisant le processeur $j$\n",
    "        \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-Calculer sa prédiction $h_{t,j}$ de ce prédicteur\n",
    " \n",
    "Créer le prédicteur $h_t$ tel que: $h_t(i) = max\\{h_{t,j}(i), j \\in \\{1,...,J\\}$\n",
    "\n",
    "Calculer la perte de $h_t$: $L_t = \\frac{1}{2} \\sum_{(i,y) \\in B} \\mu_t (i,y) (1-h_t (x_i, y_i)+h_t (x_i,y))$\n",
    "\n",
    "Calculer $\\mu_{t+1}(i,y) = \\frac{\\mu_{t}(i,y)}{C_t} \\beta_t^{w_{t,i}}$ où $\\beta_t = \\frac{L_t}{1 - L_t}$, $w_{t,i} =  \\frac{1}{2}(1-h_t (x_i, y)+h_t (x_i,y_i))$ et $C_t$ est une constante de normalisation.\n",
    "\n",
    "Retourner $h = argmax_{y \\in \\{0,...,9\\}} \\sum_{t \\in \\{1,...,T\\}} log(\\frac{1}{\\beta_t})h_t(.,y)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition de quelques fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_return_vals=2)\n",
    "def resample(probs, sample_size):\n",
    "    counts = np.random.multinomial(sample_size, probs)\n",
    "    samples = []\n",
    "    labels = []\n",
    "    samples.append(sample(counts, X_train))\n",
    "    labels.append(sample(counts, y_train))\n",
    "    return samples, labels\n",
    "\n",
    "def sample(counts, data):\n",
    "    sample = []\n",
    "    for index, count in enumerate(counts):\n",
    "        for _ in range(count):\n",
    "            sample.append(data[index])\n",
    "    return np.array(sample)\n",
    "\n",
    "@ray.remote\n",
    "def train_weak_learner(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "@ray.remote\n",
    "def make_hypothesis(model):\n",
    "    def fun(X, y):\n",
    "        return int(model.predict([X])==y)\n",
    "    return fun\n",
    "\n",
    "def merge_hypothesis(hypotheses):\n",
    "    def fun(X, y):\n",
    "        return max([hypotheses[k](X, y) for k in range(P)])\n",
    "    return fun\n",
    "\n",
    "# def final_hypothesis(learners_list, betas_list):\n",
    "#     def fun(X):\n",
    "#         return str(np.argmax(\n",
    "#             [sum([- np.log(betas_list[t]) * \n",
    "#                   learners_list[t](X, y)\n",
    "#                   for t in range(T)]) \n",
    "#              for y in classes]))\n",
    "#     return fun\n",
    "\n",
    "def make_final_classifier(hypothesis_list, betas_list):\n",
    "    def fun(X):\n",
    "        temp_classes_score = []\n",
    "        for y in classes:\n",
    "            temp_it_score = []\n",
    "            for t in range(T):\n",
    "                temp_it_score.append(betas_list[t] * hypothesis_list[t](X, y))\n",
    "            temp_classes_score.append(sum(temp_it_score))\n",
    "        return str(np.argmax(temp_classes_score))\n",
    "    return fun\n",
    "\n",
    "def update_weights(D, beta, hypothesis):\n",
    "    res = np.empty(shape=(N,), dtype=np.float32)\n",
    "    index = 0\n",
    "    for X, y in zip(X_train, y_train):\n",
    "        res[index] = D[index] * np.exp(- beta * hypothesis(X, y))\n",
    "        index += 1\n",
    "    res = res / sum(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.shape(X_train)[0]\n",
    "T = 100\n",
    "P = 10\n",
    "\n",
    "classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation de l'algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-03 18:04:45,514\tINFO resource_spec.py:212 -- Starting Ray with 3.17 GiB memory available for workers and up to 1.59 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-02-03 18:04:45,904\tINFO services.py:1093 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8266\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.31',\n",
       " 'redis_address': '192.168.1.31:11883',\n",
       " 'object_store_address': '/tmp/ray/session_2020-02-03_18-04-45_497690_87164/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-02-03_18-04-45_497690_87164/sockets/raylet',\n",
       " 'webui_url': 'localhost:8266',\n",
       " 'session_dir': '/tmp/ray/session_2020-02-03_18-04-45_497690_87164'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize boosting weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = []\n",
    "D.append(np.ones(shape=(N,), dtype=np.float32)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_X = []\n",
    "B_y = []\n",
    "for X, y in zip(X_train, y_train):\n",
    "    for char in classes:\n",
    "        if char != y:\n",
    "            B_X.append(X)\n",
    "            B_y.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(hypothesis, D):\n",
    "    epsilon = 0\n",
    "    for i in range(N):\n",
    "        neg = hypothesis(X_train[i], y_train[i])\n",
    "        coef = D[i]\n",
    "        for j in range(9):\n",
    "            epsilon += coef * (1 - neg + hypothesis(X_train[i], B_y[9*i + j]))\n",
    "    return .5 * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_pred_list(hypothesis):\n",
    "    res = []\n",
    "    for i in range(N):\n",
    "        res.append(1 - hypothesis(X_train[i], y_train[i]))\n",
    "    return np.array(res)[:, np.newaxis]\n",
    "\n",
    "def weighted_average(D, pred_list):\n",
    "    return np.dot(D, pred_list)\n",
    "\n",
    "def ada_loss(hypothesis, D):\n",
    "    return weighted_average(D, error_pred_list(hypothesis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heart of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [00:58<01:31,  1.46s/it]"
     ]
    }
   ],
   "source": [
    "hypothesis_list = []\n",
    "betas_list = []\n",
    "for t in tqdm.trange(T):\n",
    "    data_samples_id = []\n",
    "    fitted_models_id = []  # list for storing the future fitted models\n",
    "    hypotheses_id = []  # list for storing the future weak hypotheses\n",
    "    weak_learners = [tree.DecisionTreeClassifier(max_depth=4) for k in range(P)]  # list of weak learners\n",
    "    for k in range(P):\n",
    "        data_samples_id.append(resample.remote(D[-1], sample_size=N))\n",
    "    data_samples = [ray.get(data_id) for data_id in data_samples_id]\n",
    "    for k in range(P):\n",
    "        # parallelize training of weak learner k\n",
    "        model_id = train_weak_learner.remote(weak_learners[k], data_samples[k][0][0], data_samples[k][1][0])\n",
    "        # add future fitted model id\n",
    "        fitted_models_id.append(model_id)\n",
    "        # add future hypothesis is\n",
    "        hypotheses_id.append(make_hypothesis.remote(model_id))\n",
    "        \n",
    "    fitted_models = ray.get(fitted_models_id)  # get fitted weak learners\n",
    "    weak_hypotheses_list = ray.get(hypotheses_id)  # get weak hypotheses\n",
    "    hypothesis = merge_hypothesis(weak_hypotheses_list)  # compute strong hypothesis\n",
    "    hypothesis_list.append(hypothesis)  # store strong hypothesis\n",
    "    epsilon = ada_loss(hypothesis, D[-1])  # compute strong hypothesis loss\n",
    "    if epsilon == 0:\n",
    "        print(\"No error\")\n",
    "        epsilon += 0.00001\n",
    "    betas_list.append(.5 * np.log((1 - epsilon) / epsilon))\n",
    "    D.append(update_weights(D[-1], betas_list[-1], hypothesis))\n",
    "    \n",
    "final_classifier = make_final_classifier(hypothesis_list, betas_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_final = []\n",
    "for k in tqdm.trange(500):\n",
    "    res_final.append(final_classifier(X_test[k]))\n",
    "res_final = np.array(res_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test[:500], res_final))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cython",
   "language": "python",
   "name": "eltdm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
